# drift-benchmark

> A comprehensive benchmarking framework for drift detection methods

[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

**drift-benchmark** is a unified framework for evaluating and comparing drift detection methods across different datasets and scenarios. It provides a standardized interface for benchmarking various drift detection algorithms, enabling researchers and practitioners to objectively assess performance and choose the most suitable methods for their specific use cases.

**ğŸ¯ Primary Goal**: Compare how different libraries (Evidently, Alibi-Detect, scikit-learn) implement the same mathematical methods to identify which library provides better performance, accuracy, or resource efficiency for your specific use case.

## ğŸ—ï¸ Framework Architecture

**drift-benchmark** acts as a **standardization layer** that enables fair comparison of drift detection implementations across different libraries. Our framework provides:

- **ğŸ“‹ Standardized Method+Variant Definitions**: We define consistent algorithmic approaches (variants) for each mathematical method
- **âš™ï¸ Library-Agnostic Interface**: Compare how different libraries (Evidently, Alibi-Detect, scikit-learn) implement the same method+variant
- **ğŸ“Š Performance Benchmarking**: Evaluate speed, accuracy, and resource usage across implementations
- **ğŸ”„ Fair Comparisons**: Ensure all libraries are tested under identical conditions and data preprocessing

### ğŸ“š Core Concepts

- **ğŸ”¬ Method**: Mathematical methodology for drift detection (e.g., Kolmogorov-Smirnov Test, Maximum Mean Discrepancy)
- **âš™ï¸ Variant**: Standardized algorithmic approach defined by drift-benchmark (e.g., batch processing, incremental processing, sliding window)
- **ğŸ”Œ Detector**: How a specific library implements a method+variant combination (e.g., Evidently's KS batch vs. Alibi-Detect's KS batch)
- **ğŸ”„ Adapter**: Your custom class that maps a library's implementation to our standardized method+variant interface

### ğŸ¯ Framework Roles

#### **For drift-benchmark developers (us):**

- Design and maintain the `methods.toml` registry that standardizes drift detection methods across libraries
- Provide the `BaseDetector` abstract interface for consistent detector integration
- Implement core benchmarking infrastructure (data loading, execution, results)

#### **For end users (researchers/practitioners):**

- Create **adapter classes** by extending `BaseDetector` to integrate their preferred drift detection libraries
- Configure benchmarks using our standardized method+variant identifiers
- Run comparative evaluations across different detectors and datasets

### ğŸ”„ Integration Flow

```mermaid
graph TD
    A[methods.toml Registry] --> B[Method + Variant Definition]
    B --> C[User Creates Adapter Class]
    C --> D[Extends BaseDetector Interface]
    D --> E[Registers with @register_detector]
    E --> F[Available for Benchmarking]

    G[User's Detection Library] --> C
    H[Evidently/scikit-learn/River] --> C
```

**Key Insight**: Libraries like Evidently or Alibi-Detect don't define variants themselves. Instead, **drift-benchmark defines standardized variants** (like "batch" or "incremental"), and users create adapters that map their library's specific implementation to match our variant specifications.## ğŸ¯ Features

### Core Capabilities

- **ğŸ“‹ Standardized Registry**: Curated `methods.toml` defining mathematical methods and their algorithmic variants
- **ğŸ”Œ Unified Interface**: Consistent `BaseDetector` API for integrating any drift detection library
- **ğŸ“Š Flexible Data Handling**: Support for pandas DataFrames and automatic conversion to library-specific formats
- **ğŸ“ˆ Comprehensive Evaluation**: Performance metrics including accuracy, precision, recall, and execution time
- **ğŸ—‚ï¸ Multiple Data Types**: Support for continuous, categorical, and mixed data types
- **âš™ï¸ Configurable Benchmarks**: TOML-based configuration for reproducible experiments

### Supported Drift Types

- **Covariate Drift**: Changes in input feature distributions
- **Concept Drift**: Changes in the relationship between features and target
- **Prior Drift**: Changes in target variable distribution

### Data Format Support

- **csv Files**: Standard comma-separated values with automatic type inference
- **Univariate & Multivariate**: Support for single and multiple feature scenarios
- **Flexible Splits**: Configurable reference/test data splitting

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/BorjaEst/drift-benchmark.git
cd drift-benchmark

# Install dependencies
pip install -r requirements.txt

# Install in development mode
pip install -e .
```

### Basic Usage

#### 1. Configuration Setup

Create a benchmark configuration file (`benchmark_config.toml`):

```toml
[[datasets]]
path = "datasets/example.csv"
format = "csv"
reference_split = 0.5

# Compare different library implementations of the same method+variant
[[detectors]]
method_id = "kolmogorov_smirnov"
variant_id = "batch"
library_id = "evidently"

[[detectors]]
method_id = "kolmogorov_smirnov"
variant_id = "batch"
library_id = "alibi-detect"

# Also compare different methods
[[detectors]]
method_id = "cramer_von_mises"
variant_id = "batch"
library_id = "scipy"
```

#### 2. Run Benchmark

```python
from drift_benchmark import BenchmarkRunner

# Load configuration and run benchmark
runner = BenchmarkRunner.from_config_file("benchmark_config.toml")
results = runner.run()

# Results are automatically saved to timestamped directory
print(f"Results saved to: {results.output_directory}")
```

#### 3. Compare Library Performance

```python
# Access individual detector results to compare implementations
for result in results.detector_results:
    print(f"Library: {result.library_id}")
    print(f"Method+Variant: {result.method_id}_{result.variant_id}")
    print(f"Dataset: {result.dataset_name}")
    print(f"Drift Detected: {result.drift_detected}")
    print(f"Execution Time: {result.execution_time:.4f}s")
    print(f"Drift Score: {result.drift_score}")
    print("---")

# Example output:
# Library: evidently
# Method+Variant: kolmogorov_smirnov_batch
# Execution Time: 0.0234s
# ---
# Library: alibi-detect
# Method+Variant: kolmogorov_smirnov_batch
# Execution Time: 0.0156s  <- Alibi-Detect is faster!
# ---

# View summary statistics
summary = results.summary
print(f"Total Detectors: {summary.total_detectors}")
print(f"Successful Runs: {summary.successful_runs}")
print(f"Failed Runs: {summary.failed_runs}")
print(f"Average Execution Time: {summary.avg_execution_time:.4f}s")
```

## ğŸ“Š Architecture

### Module Structure

```text
src/drift_benchmark/
â”œâ”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ settings.py              # Configuration management
â”œâ”€â”€ exceptions.py            # Custom exception classes
â”œâ”€â”€ literals.py              # Type definitions and enums
â”œâ”€â”€ models/                  # Pydantic data models
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ configurations.py    # Config models (BenchmarkConfig, DatasetConfig)
â”‚   â”œâ”€â”€ results.py          # Result models (BenchmarkResult, DetectorResult)
â”‚   â””â”€â”€ metadata.py         # Metadata models (DatasetMetadata, DetectorMetadata)
â”œâ”€â”€ detectors/              # Method registry and metadata
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ methods.toml        # Standardized method+variant definitions
â”‚   â””â”€â”€ registry.py         # Method loading and lookup
â”œâ”€â”€ adapters/               # Detector interface framework
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base_detector.py    # BaseDetector abstract class
â”‚   â””â”€â”€ registry.py         # Detector registration system
â”œâ”€â”€ data/                   # Data loading utilities
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ file_loader.py      # File loading utilities
â”œâ”€â”€ config/                 # Configuration loading
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ loader.py           # TOML configuration parsing
â”œâ”€â”€ benchmark/              # Benchmark execution
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core.py             # Benchmark class
â”‚   â””â”€â”€ runner.py           # BenchmarkRunner class
â””â”€â”€ results/                # Result storage and export
    â”œâ”€â”€ __init__.py
    â””â”€â”€ storage.py          # Result saving and export
```

### Data Flow Pipeline

1. **Configuration Loading**: Parse TOML configuration files with validation
2. **Dataset Loading**: Load csv files and split into reference/test sets
3. **Detector Setup**: Instantiate configured detectors from registry
4. **Benchmark Execution**:
   - **Preprocessing**: Convert data to detector-specific formats
   - **Training**: Fit detectors on reference data
   - **Detection**: Run drift detection on test data
   - **Scoring**: Collect performance metrics
5. **Result Storage**: Export results to timestamped directories

## ğŸ§ª Adding New Detectors

The power of drift-benchmark comes from comparing how different libraries implement the same method+variant. Here's how to create adapters for comparison:

### 1. Create Multiple Adapters for the Same Method+Variant

```python
from drift_benchmark.adapters import BaseDetector, register_detector
import numpy as np
from evidently.metrics import DataDriftPreset
from alibi-detect.cd import KSDrift

# Evidently's implementation of KS batch variant
@register_detector(method_id="kolmogorov_smirnov", variant_id="batch", library_id="evidently")
class EvidentlyKSDetector(BaseDetector):
    """Evidently's implementation of Kolmogorov-Smirnov batch processing."""

    def __init__(self, method_id: str, variant_id: str, **kwargs):
        super().__init__(method_id, variant_id)
        self.threshold = kwargs.get('threshold', 0.05)
        self._detector = None

    def preprocess(self, data, **kwargs):
        """Convert to Evidently's expected format"""
        return data.X_ref.values if 'X_ref' in str(data) else data.X_test.values

    def fit(self, preprocessed_data, **kwargs):
        # Evidently's setup for KS test
        self._reference_data = preprocessed_data
        return self

    def detect(self, preprocessed_data, **kwargs):
        # Evidently's KS implementation
        # Implementation details here...
        return drift_detected

# Alibi-Detect's implementation of the same KS batch variant
@register_detector(method_id="kolmogorov_smirnov", variant_id="batch", library_id="alibi-detect")
class AlibiDetectKSDetector(BaseDetector):
    """Alibi-Detect's implementation of Kolmogorov-Smirnov batch processing."""

    def __init__(self, method_id: str, variant_id: str, **kwargs):
        super().__init__(method_id, variant_id)
        self.threshold = kwargs.get('threshold', 0.05)

    def preprocess(self, data, **kwargs):
        """Convert to Alibi-Detect's expected format"""
        return data.X_ref.values if 'X_ref' in str(data) else data.X_test.values

    def fit(self, preprocessed_data, **kwargs):
        # Alibi-Detect's KS detector setup
        self._detector = KSDrift(preprocessed_data, p_val=self.threshold)
        return self

    def detect(self, preprocessed_data, **kwargs):
        # Alibi-Detect's KS implementation
        result = self._detector.predict(preprocessed_data)
        return result['data']['is_drift']
```

**Result**: Now you can benchmark **Evidently vs. Alibi-Detect** on the exact same KS batch variant to see which is faster/more accurate!

### 2. Verify Method Definition in methods.toml

Check that the method and variant are already defined in our standardized registry:

```toml
[methods.kolmogorov_smirnov]
name = "Kolmogorov-Smirnov Test"
description = "Two-sample test for equality of continuous distributions"
drift_types = ["covariate"]
family = "statistical-test"
data_dimension = "univariate"
data_types = ["continuous"]
requires_labels = false
references = ["https://doi.org/10.2307/2281868", "Massey Jr. (1951)"]

[methods.kolmogorov_smirnov.variants.custom]
name = "Custom Implementation Variant"
execution_mode = "batch"
hyperparameters = ["threshold"]
references = ["Your implementation reference"]
```

## ğŸ“ˆ Results and Metrics

### Output Structure

Each benchmark run creates a timestamped directory with:

```text
results/20250720_143022/
â”œâ”€â”€ benchmark_results.json   # Complete results in JSON format
â”œâ”€â”€ config_info.toml        # Configuration used for reproducibility
â””â”€â”€ benchmark.log           # Execution log
```

### Performance Metrics

- **Execution Time**: Measured using `time.perf_counter()` with second precision
- **Success Rate**: Ratio of successful detector runs
- **Drift Detection**: Binary drift detection results
- **Drift Scores**: Continuous scores when available (detector-dependent)

### Summary Statistics

When ground truth is available:

- **Accuracy**: Correct drift detection rate
- **Precision**: True positive rate among positive predictions
- **Recall**: True positive rate among actual positives

## ğŸ”§ Configuration

### Environment Variables

All settings can be configured via environment variables with `DRIFT_BENCHMARK_` prefix:

```bash
export DRIFT_BENCHMARK_DATASETS_DIR="./datasets"
export DRIFT_BENCHMARK_RESULTS_DIR="./results"
export DRIFT_BENCHMARK_LOG_LEVEL="info"
export DRIFT_BENCHMARK_RANDOM_SEED=42
```

### Settings

| Setting                 | Default                                        | Description                                           |
| ----------------------- | ---------------------------------------------- | ----------------------------------------------------- |
| `datasets_dir`          | `"datasets"`                                   | Directory for dataset files                           |
| `results_dir`           | `"results"`                                    | Directory for benchmark results                       |
| `logs_dir`              | `"logs"`                                       | Directory for log files                               |
| `log_level`             | `"info"`                                       | Logging level (debug, info, warning, error, critical) |
| `random_seed`           | `42`                                           | Random seed for reproducibility                       |
| `methods_registry_path` | `"src/drift_benchmark/detectors/methods.toml"` | Path to methods configuration                         |

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

### Development Setup

```bash
# Clone repository
git clone https://github.com/BorjaEst/drift-benchmark.git
cd drift-benchmark

# Install development dependencies
pip install -r requirements-dev.txt

# Install pre-commit hooks
pre-commit install

# Run tests
pytest tests/

# Check code style
black src/ tests/
flake8 src/ tests/
```

### Testing

The project follows Test-Driven Development (TDD) principles:

```bash
# Run all tests
pytest

# Run specific test modules
pytest tests/test_adapters/
pytest tests/test_models/

# Run with coverage
pytest --cov=src/drift_benchmark tests/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ”— Links

- **Documentation**: [Coming Soon]
- **Issue Tracker**: [GitHub Issues](https://github.com/BorjaEst/drift-benchmark/issues)
- **Discussions**: [GitHub Discussions](https://github.com/BorjaEst/drift-benchmark/discussions)

## ğŸ™ Acknowledgments

- [scikit-learn](https://scikit-learn.org/) for statistical methods
- [River](https://riverml.xyz/) for online learning algorithms
- [Evidently](https://www.evidentlyai.com/) for data drift monitoring
- [Pydantic](https://pydantic-docs.helpmanual.io/) for data validation

## ğŸ“ Citation

If you use drift-benchmark in your research, please cite:

```bibtex
@software{drift_benchmark_2025,
  title = {drift-benchmark: A Comprehensive Framework for Drift Detection Benchmarking},
  author = {BorjaEst},
  year = {2025},
  url = {https://github.com/BorjaEst/drift-benchmark}
}
```

---

**Status**: ğŸš§ Under Active Development

This project is currently in development. The API may change between versions. Please check the [CHANGELOG](CHANGELOG.md) for updates.
