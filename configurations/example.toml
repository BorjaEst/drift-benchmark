# Benchmark Configuration Example

[metadata]
    name        = "Example Drift Detection Benchmark"
    description = "A simple benchmark to demonstrate the capabilities of drift-benchmark"
    author      = "Drift Benchmark Team"
    date        = "2025-06-18"
    version     = "0.1.0"

[settings]
    seed                 = 42
    n_runs               = 5
    cross_validation     = true
    cv_folds             = 3
    timeout_per_detector = 300 # seconds

[data]
# Dataset configurations
[[data.datasets]]
    name           = "synthetic_gradual_drift"
    type           = "synthetic"
    n_samples      = 5000
    n_features     = 20
    drift_type     = "gradual"
    drift_position = 0.5
    noise          = 0.05

[[data.datasets]]
    name           = "synthetic_sudden_drift"
    type           = "synthetic"
    n_samples      = 5000
    n_features     = 20
    drift_type     = "sudden"
    drift_position = 0.6
    noise          = 0.05

[[data.datasets]]
    name       = "california_housing"
    type       = "builtin"
    test_size  = 0.3
    preprocess = { scaling = true, scaling_method = "standard", handle_missing = true }

[[data.datasets]]
    name          = "real_weather"
    type          = "file"
    path          = "datasets/weather.csv"
    target_column = "RainTomorrow"
    drift_column  = "date"
    train_size    = 0.3


[detectors]
# List of detectors to evaluate
[[detectors.algorithms]]
    name       = "KSDrift"
    library    = "alibi_detect_adapter"
    parameters = { p_val_threshold = 0.05, alternative = "two-sided" }

[[detectors.algorithms]]
    name       = "MMDDrift"
    library    = "alibi_detect_adapter"
    parameters = { p_val_threshold = 0.05, kernel = "rbf", backend = "numpy", n_permutations = 100 }

[[detectors.algorithms]]
    name       = "ChiSquareDrift"
    library    = "alibi_detect_adapter"
    parameters = { p_val_threshold = 0.05 }

[[detectors.algorithms]]
    name       = "FeatureDrift"
    library    = "evidently_adapter"
    parameters = { column_name = "feature_1", significance_level = 0.01 }


[metrics]
    # Evaluation metrics
    metrics = [
        "detection_delay",
        "false_positive_rate",
        "false_negative_rate",
        "f1_score",
        "computation_time",
    ]

[output]
    save_results  = true
    results_dir   = "results/example_benchmark"
    visualization = true
    plots         = ["roc_curve", "delay_distribution", "performance_comparison"]
    export_format = ["csv", "json"]
    log_level     = "info"
